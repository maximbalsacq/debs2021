# About
The code in this repository aims to solve the
[DEBS Grand Challenge 2021](https://2021.debs.org/call-for-grand-challenge-solutions/).


# Usage
## Rust Installation
This project is written in the Rust programming language.
If the Rust-specific is not yet installed on your machine,
visit the [rustup](https://rustup.rs/) website and follow the installation instructions.

## Setup and Configuration
Download the data from [the shared folder](https://bwsyncandshare.kit.edu/s/caadGD4AKiHbCPR)
and save it into the same folder. The total dataset consists of about 32 GiB of data.
The folder will be considered the root folder of the data. 

For some binaries, notably the q1 binary, `DEBS_DATA_ROOT` needs to be set
to this directory before executing the binary:
```sh
export DEBS_DATA_ROOT="/path/to/extracted/data"
```

The data was split into smaller chunks to avoid having a multi-gigabyte file
(which seems to cause issues with nextcloud).
To extract the messages, join the parts with:
```sh
cat messages.x* > messages.tar.gz
```

Then extract it using:
```sh
tar xf messages.tar.gz
```

If required, the archive files can now be removed to free disk space.

## Building
### Release mode (preferred)
For the best performance, compile in release mode:
```sh
cargo build --release
```

No errors should be shown, and binaries generated in `target/release`.

### Debug mode (for development)
In debug mode, some extra assertions
are enabled which verify that the result of
optimizations is correct. As the query code is
strongly based on functional programming and relies
heavily compiler optimization, optimizations have been
enabled even in debug mode.
```sh
cargo build
```

## Running
### To solve query 1
After building, run
```sh
./target/release/q1
```

Note that `DEBS_DATA_ROOT` needs to be set
(see [Setup and Configuration](#setup-and-configuration)).

Currently, the program only outputs messages for every 5 minutes
of data processed and (although generated) does not output or
store the solution somewhere. Writing the results
somewhere is still on the todo list.


### Other binaries
These binaries are not necessary to solve the query,
but have been developed in the scope of this project.

#### download
The download binary was used to download the given dataset
from the DEBS servers. As the DEBS servers are offline,
this program is now only useful as a reference for how
a client would have looked like.

#### `analyze_locations`/`analyze_measurements`
Both programs analyze various featurees of the corresponding
data files and output them to stdout. They were used to
explore the data and find potential edge cases or possibilities
for performance. For details, see their source code.


#### `gen_smaller_dataset`
Generates a small test dataset which is used
to test the functionality and performance of
the location process. Require `DEBS_DATA_ROOT`
to be set correctly.

#### geo2sql/batch2ql
geo2sql and batch2sql convert location info and
batch data to SQL, respectively.
The generated SQL files use the schema created by the following

```sql
CREATE TABLE locations(postalcode text primary key, city text not null, geog geometry(multipolygon));
CREATE TABLE meas_current(batchnum integer, at timestamp, location geometry(point), p1 double precision, p2 double precision);
CREATE TABLE meas_lastyear(batchnum integer, at timestamp, location geometry(point), p1 double precision, p2 double precision);
```

In the future, SQL statements replicating the functionality of query 1
will be implemented/added to verify the output of the q1 binary.


# Development
## Testing
To run tests, use
```sh
cargo test
```
Note that two tests require `DEBS_DATA_ROOT` to be set correctly
(see [Setup and Configuration](#setup-and-configuration)).

### `aggregate::particle_aggregate_test::test_aggregate_multiple`
### `aggregate::particle_aggregate_test::test_from_f32`
TODO

### `aggregate::window_test::join_test`
### `aggregate::window_test::different_windowsizes_test`
### `aggregate::window_test::split_join_test`
### `spliter::spliter_test::simple_spliter_test`
TODO

### `aqi::tests::test_valid_pm10_aqis` / `aqi::tests::test_valid_pm25_aqis`
These tests compare automatically calculated AQI values with
manually calculated ones to verify the AQI calculation works correctly.

### `aqi::tests::test_all_pm25_aqis_work`/`aqi::tests::test_all_pm10_aqis_work`
As the range of valid AQI values is based on the quite small range of valid
input values for particle concentration values, these tests check
exhaustively that a corresponding AQI value can be constructed for every
input valid value.



### `tests::check_locating_works`
Checks that
- A test coordinate outside germany is not found
- A second test coordinate (here, Freiburg) is found
- The second test coordinate only matches one city

Note: This test requires that `DEBS_DATA_ROOT` is correctly set.

### `tests::check_locating_samples_works`
Using the test dataset generated by the `gen_smaller_dataset` binary,
this test checks if exactly one location can be found for every messsage
in the test data set.

Note: This test requires that `DEBS_DATA_ROOT` is correctly set.



## Benchmarking
Currently, only one benchmark exsists,
used to analyze the speed of matchin a pair of coordinates to a city.
Use
```sh
cargo bench
```
to run all benchmarks.


# Architecture
TODO
